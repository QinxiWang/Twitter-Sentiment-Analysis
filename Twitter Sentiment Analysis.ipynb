{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qinxi/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining Sentiment on Twitter\n",
    "\n",
    "\n",
    "### Overview\n",
    "\n",
    "I experiement with four classifiers, and here are the overall results:\n",
    "\n",
    "    unigram(tf-idf) - GradientBoosting : 70.7%\n",
    "    bigram(tf-idf) - GradientBoosting : 62.7%\n",
    "    word2vec - GradientBoosting : 61.3%\n",
    "    LSTM (2 layer) on 12 epochs: 93.6%\n",
    "\n",
    "\n",
    "### Part 1: Streaming tweets \n",
    "\n",
    "I streamed 'happy' and 'sad' tweets using the streaming API with the aid of the search API for historical data, as there are less tweets with \":(\" than there are with \":)\". I also set the language to be English('en') only and the region to be in the United States. Overall I have 29521 positive tweets, and 35422 negative tweets.\n",
    "\n",
    "The code for streaming is in the appendex. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "happy = pd.read_csv(\"./positive-emo_2018-03-08.json\", sep=\"\\n\", error_bad_lines=False, warn_bad_lines=False, engine='python')\n",
    "happy.columns = [\"tweets\", \"json\"]\n",
    "happy = happy.drop(['json'], axis=1)\n",
    "sad = pd.read_csv(\"negative-emo_2018-03-04_to_2018-03-09.json\", error_bad_lines=False, warn_bad_lines=False, engine='python')\n",
    "sad.columns = [\"tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29521 positive tweets\n",
      "There are 35422 negative tweets\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(happy), \"positive tweets\")\n",
    "print(\"There are\", len(sad), \"negative tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part 2: Tweets cleanup\n",
    "\n",
    "Now, we add in labels to mark the tweets as \":(\" or \":)\" for happy and sad faces. Then we merge the two datasets into one. \n",
    "\n",
    "Also we prepare the stopwords from nltk package, along with the symbols we see a lot in tweets - we add then into a set called stopws, along with the \":)\" and \":(\" emoticon from the tweets. Also nltk has a TweetTokenizer that helps with the clean up process.\n",
    "\n",
    "Then we use a for loop to go through each tweets, using regex and stopwords we remove websites, usernames, emoticons, reweets etc; convert to all lowercases, and finally use the tokenize the tweets into lists of words/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "happy['emo'] = \":)\"\n",
    "sad['emo'] = \":(\"\n",
    "tweet = happy.merge(sad, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopws = stopwords.words(\"english\")\n",
    "stopws = set(stopws)\n",
    "stopws.add(\"rt\")\n",
    "stopws.add(\":)\")\n",
    "stopws.add(\":(\")\n",
    "stopws.add(\":\")\n",
    "stopws.add(\",\")\n",
    "stopws.add(\".\")\n",
    "stopws.add(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>emo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[well, means, with, guys, and, pizza, on, firs...</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[circle, giveaway, give, appreciation, an, awe...</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[trump, so, strong, wonder, cnn, find, make, s...</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[aksener, sanada, pembe, zarfta, koyarlar, par...</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[hi, guys, m, writing, paper, my, public, rela...</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets emo\n",
       "0  [well, means, with, guys, and, pizza, on, firs...  :)\n",
       "1  [circle, giveaway, give, appreciation, an, awe...  :)\n",
       "2  [trump, so, strong, wonder, cnn, find, make, s...  :)\n",
       "3  [aksener, sanada, pembe, zarfta, koyarlar, par...  :)\n",
       "4  [hi, guys, m, writing, paper, my, public, rela...  :)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove posts that contain both happy and sad emo\n",
    "#Remove stop words, websites (\"http...\"), usernames (\"@shilad\"), and anything else that seems weird to you.\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "for i in range(len(tweet.tweets)):\n",
    "    tweet.tweets[i] = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", tweet.tweets[i]).split())\n",
    "    tweet.tweets[i] = tknzr.tokenize(tweet.tweets[i])    \n",
    "    for word in tweet.tweets[i]: # iterate over word_list\n",
    "        if word in stopws: \n",
    "            tweet.tweets[i].remove(word)\n",
    "\n",
    "tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the tokenize text data, we can go ahead and calculate the frequeny matrices for postive and negative tweets separately. \n",
    "\n",
    "\n",
    "### Part 3: Frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the frequency matrix.\n",
    "\n",
    "pos_counts = defaultdict(int)\n",
    "neg_counts = defaultdict(int)\n",
    "for i in range(len(tweet)):    \n",
    "    if tweet.emo[i] == \":)\":\n",
    "        for token in tweet.tweets[i]:\n",
    "            pos_counts[token] += 1\n",
    "    else:  #emo == \":(\"\n",
    "        for token in tweet.tweets[i]:\n",
    "            neg_counts[token] += 1\n",
    "    \n",
    "    pos_total = sum(pos_counts.values())\n",
    "    neg_total = sum(neg_counts.values())\n",
    "    \n",
    "    for token in pos_counts:\n",
    "        pos_counts[token] /= pos_total\n",
    "    for token in neg_counts:\n",
    "        neg_counts[token] /= neg_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(\"The frequency matrix for postive tweets is \", pos_counts)\n",
    "#print(\"The frequency matrix for negative tweets is \", neg_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Output too long, didn't print. \n",
    "\n",
    "\n",
    "I realize that later on the CountVectorizer function only takes in strings, so I add in one more column that contains the string of tweets separated by space. So now our data frame looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>emo</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[well, means, with, guys, and, pizza, on, firs...</td>\n",
       "      <td>:)</td>\n",
       "      <td>well means with guys and pizza on first name b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[circle, giveaway, give, appreciation, an, awe...</td>\n",
       "      <td>:)</td>\n",
       "      <td>circle giveaway give appreciation an awesome d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[trump, so, strong, wonder, cnn, find, make, s...</td>\n",
       "      <td>:)</td>\n",
       "      <td>trump so strong wonder cnn find make sound lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[aksener, sanada, pembe, zarfta, koyarlar, par...</td>\n",
       "      <td>:)</td>\n",
       "      <td>aksener sanada pembe zarfta koyarlar pardon oy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[hi, guys, m, writing, paper, my, public, rela...</td>\n",
       "      <td>:)</td>\n",
       "      <td>hi guys m writing paper my public relations cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets emo  \\\n",
       "0  [well, means, with, guys, and, pizza, on, firs...  :)   \n",
       "1  [circle, giveaway, give, appreciation, an, awe...  :)   \n",
       "2  [trump, so, strong, wonder, cnn, find, make, s...  :)   \n",
       "3  [aksener, sanada, pembe, zarfta, koyarlar, par...  :)   \n",
       "4  [hi, guys, m, writing, paper, my, public, rela...  :)   \n",
       "\n",
       "                                                text  \n",
       "0  well means with guys and pizza on first name b...  \n",
       "1  circle giveaway give appreciation an awesome d...  \n",
       "2  trump so strong wonder cnn find make sound lik...  \n",
       "3  aksener sanada pembe zarfta koyarlar pardon oy...  \n",
       "4  hi guys m writing paper my public relations cl...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet['text'] = 0\n",
    "for i in range(len(tweet)):\n",
    "    tweet.text[i] = ' '.join(tweet.tweets[i])\n",
    "\n",
    "tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Build Sentiment Classifier\n",
    "\n",
    "Now we can train our classifier. First we split the data into 80% training data and the rest 20% testing data.\n",
    "\n",
    "Then we use CountVectorizer and a TfidfTransformer functions to extract the Tf-Idf matrix of the training data, fit into the GradientBoosting Classifier, and use the confusion matrix to show the accuracy rate, which is about 70.7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split your tweets into training data (80% of your data) and testing data (20% of your data)\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(tweet.text)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "\n",
    "(X_tfidf_train, X_tfidf_test, Y_train, Y_test) = train_test_split(X_tfidf, \n",
    "                                                                  tweet.emo, \n",
    "                                                                  test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train your sentiment analysis classifier as we did in class, using the Tf-Idf matrix\n",
    "gbc = GradientBoostingClassifier().fit(X_tfidf_train, Y_train)\n",
    "Y_pred_uni = gbc.predict(X_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6365,  623],\n",
       "       [3185, 2816]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the confusion matrix for unigram\n",
    "confusion_matrix(Y_test, Y_pred_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is  0.7068288551851567\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy is \", (6365+2816)/(6365+2816+623+3185))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to experiment with bag of 2 words instead, so we use a TfidfVectorizer, which is similar to a functor of CountVectorizer follow by a TfidfTransformer. We set the ngram_range to be exact 2, and perform the same procedure as above. \n",
    "\n",
    "Yet, the accuracy rate is only 62.7%, which is actually 8% worse than just using unigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Experiment with using bi-grams. Do they help?\n",
    "bigram_vect_tfidf = TfidfVectorizer(ngram_range=(2, 2))\n",
    "X_bigram = bigram_vect_tfidf.fit_transform(tweet.text)\n",
    "\n",
    "(X_bigram_train, X_bigram_test, Y_bigram_train, Y_bigram_test) = train_test_split(X_bigram, \n",
    "                                                                  tweet.emo, \n",
    "                                                                  test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbc_bigram = GradientBoostingClassifier().fit(X_bigram_train, Y_bigram_train)\n",
    "Y_pred_bi = gbc_bigram.predict(X_bigram_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6943,   45],\n",
       "       [4806, 1195]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix for bigram\n",
    "confusion_matrix(Y_bigram_test, Y_pred_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is  0.626530140888444\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy is \", (6943+1195)/(6943+1195+45+4806))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Sentiment Classification in Action\n",
    "\n",
    "Since we were querying the tweets using emoticons, we are training on a biased data. We are wondering how the trained classifier would perform on tweets without emoticons.\n",
    "\n",
    "I decided to use the hashtag #MeToo, and queried the 14016 tweets in a 100 sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Collect a stream of at least 1000 tweets and predict the sentiment of each\n",
    "\n",
    "hashtag = pd.read_csv(\"hashtag.json\", error_bad_lines=False, warn_bad_lines=False, engine='python')\n",
    "hashtag.columns = [\"tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14016 tweets with #MeToo tag\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(hashtag), \"tweets with #MeToo tag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the same procedure as above to do data cleaning - remove websites, retweets, usernames, etc. And add in the text column into the data frame, and our hashtag data looks like the following after clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qinxi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/Users/qinxi/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[wanted, get, mad, this, she, right, i, was, a...</td>\n",
       "      <td>wanted get mad this she right i was always wok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[we, u2019re, hiring, front, end, dev, join, e...</td>\n",
       "      <td>we u2019re hiring front end dev join engineeri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[u201csexual, violence, knows, race, class, ge...</td>\n",
       "      <td>u201csexual violence knows race class gender t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[u6027, u66b4, u529b, u88ab, u5bb3, u3092, u54...</td>\n",
       "      <td>u6027 u66b4 u529b u88ab u5bb3 u3092 u544a u767...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[hey, please, help, spread, word, s, latest, f...</td>\n",
       "      <td>hey please help spread word s latest film shin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  \\\n",
       "0  [wanted, get, mad, this, she, right, i, was, a...   \n",
       "1  [we, u2019re, hiring, front, end, dev, join, e...   \n",
       "2  [u201csexual, violence, knows, race, class, ge...   \n",
       "3  [u6027, u66b4, u529b, u88ab, u5bb3, u3092, u54...   \n",
       "4  [hey, please, help, spread, word, s, latest, f...   \n",
       "\n",
       "                                                text  \n",
       "0  wanted get mad this she right i was always wok...  \n",
       "1  we u2019re hiring front end dev join engineeri...  \n",
       "2  u201csexual violence knows race class gender t...  \n",
       "3  u6027 u66b4 u529b u88ab u5bb3 u3092 u544a u767...  \n",
       "4  hey please help spread word s latest film shin...  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hashtag tweets clean up\n",
    "\n",
    "for i in range(len(hashtag.tweets)):\n",
    "    hashtag.tweets[i] = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", hashtag.tweets[i]).split())\n",
    "    hashtag.tweets[i] = tknzr.tokenize(hashtag.tweets[i])    \n",
    "    for word in hashtag.tweets[i]:\n",
    "        if word in stopws: \n",
    "            hashtag.tweets[i].remove(word)\n",
    "\n",
    "hashtag['text'] = 0\n",
    "for i in range(len(hashtag)):\n",
    "    hashtag.text[i] = ' '.join(hashtag.tweets[i])\n",
    "\n",
    "hashtag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vocab = count_vect.transform(tweet.text)\n",
    "count_vect2 = CountVectorizer(vocabulary=count_vect.vocabulary_)\n",
    "hashtag_counts = count_vect2.fit_transform(hashtag.text)\n",
    "hashtag_tfidf = tfidf_transformer.fit_transform(hashtag_counts)\n",
    "hashtag_pred = gbc.predict(hashtag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([':)', ':)', ':(', ..., ':(', ':)', ':('], dtype=object)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking through the results, the performance of our classifier on the new dataset is worse than the predictions on our original datasets. Given the length of the new tweets I shall not print out the actual tweets, but have identified the following patterns:\n",
    "\n",
    "1. The classifier tend to identify netural tweets or tweets with the hashtag but unrelated content as positive.\n",
    "2. I might have not specified a correct encoder so there are many words that has a format similar to 'u00b4', and the classifier seems to tag it as positive for the most of the time.\n",
    "3. Tweets show vulnerability but not negative seems to be classify as negative\n",
    "4. Tweets that people accknowledge negative feelings/things but then show postive affirmations also tend to be labeled as negative. \n",
    "\n",
    "Overall there is no clear trend of how the classifier performs, and I have listed the top words that the classier has relied on predicting, other than some obvious positive words such as 'happy', 'great', 'good', 'awesome', the dictionary itself is not good enough with the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u00b4', 'u2026', 'miss', 'yoongi', 'thanks', 'he', 'gracias', 'thank', 'cute', 'hemos', 'que', 'eu', 'you', 'sad', 'hermanas', 'happy', 'great', 'feliz', 'seokjin', 'day', 'how', 'stans', 'sorry', 'video', 'u0e19', 'quiero', 'queria', 'bts', 'ub2e4', 'quero', 'remember', 'smash', 'triste', 'weloveyouseokjin', 'uac', 'hi', 'ud558', 'u2019s', 'wish', 'u0e2d', 'like', 'internationalwomensday', 'love', 'u00e9', 'gt', 'u00e3o', 'good', 'retweet', 'so', 'uff', 'jin', 'jackson', 'uae', 'your', 'the', 'u0e22', 'era', 'check', 'new', 'smol', 'u00f1o', 'un', 'u2019t', 'alg', 'podem', 'para', 'oi', 'my', 'u0644', 'yo', 'u306a', 'abra', 'look', 'awesome', 'htt', 'know', 'switch', 'lt', 'rt', 'guys', 'uc774', 'women', 'today', 'necesito', 'welcome', 'ko', 'his', 'kca', 'meu', 'want', 'tweet', 'u06cc', 'us', 'pero', 'stop', 'btsarmy', 'u00fan', 'u0e29', 'de', 'birthday']\n"
     ]
    }
   ],
   "source": [
    "#hashtag.text\n",
    "top_indexes = gbc.feature_importances_.argsort()[::-1]\n",
    "names = count_vect.get_feature_names()\n",
    "print([names[i] for i in top_indexes[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: NPL and Deep Learning approaches: \n",
    "\n",
    "### Part 1: Word2Vec model \n",
    "\n",
    "After the experiment with the tf-idf matrix, we wonder if word2vec would perform better. We train a Word2Vec model using the gensim package by getting the word vectors of each tweets, then normalize on the column by column, and then build the vector using the word freqency from all the tweets in our data.\n",
    "\n",
    "\n",
    "Again, we split the tweets into 80% training data and 20% testing data. But the result only yields a 61.3% accuracy. One observation though, the word2vec model seems to be able to predict the negative tweets better than the previous tf-idf martics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "import gensim\n",
    "import logging\n",
    "import os.path\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = gensim.models.Word2Vec(tweet.tweets, workers=4, size=50, sg=1, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qinxi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if __name__ == '__main__':\n",
      "/Users/qinxi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231081 130107\n"
     ]
    }
   ],
   "source": [
    "tweet_vectors = []\n",
    "\n",
    "num_known_words = 0\n",
    "num_unknown_words = 0\n",
    "for t in tweet.text:    \n",
    "    v = np.zeros(50)\n",
    "    n = 0\n",
    "    for word in gensim.utils.tokenize(t, lowercase=True):\n",
    "        if word in model:\n",
    "            v += model[word]\n",
    "            n += 1\n",
    "            num_known_words += 1\n",
    "        else:\n",
    "            num_unknown_words += 1\n",
    "    if n > 0: v /= n # average the word vectors\n",
    "    tweet_vectors.append(v)\n",
    "print(num_known_words, num_unknown_words)\n",
    "\n",
    "tweet_w2v = np.array(tweet_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_w2v_train, X_w2v_test, Y_w2v_train, Y_w2v_test) = train_test_split(tweet_w2v, \n",
    "                                                              tweet.emo, \n",
    "                                                              test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbc_w2v = GradientBoostingClassifier().fit(X_w2v_train, Y_w2v_train)\n",
    "Y_pred_w2v = gbc_w2v.predict(X_w2v_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5548, 1440],\n",
       "       [2027, 3974]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show confusion matrix for \n",
    "confusion_matrix(Y_w2v_test, Y_pred_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is  0.6134123558590479\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy is \", (5548+3974)/(5548+3974+2027+3974))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: LSTM model\n",
    "\n",
    "Disappointed in the previous accuracy only stayed at low 70%, I wonder whether a neural network would perform better. \n",
    "\n",
    "Few years ago I heard the state of the art for sentiment classification is using Tree-LSTM model, given the limited computational power in my personal computer, I decided to go with a single LSTM model instead.\n",
    "\n",
    "The implementation of the code, and especially the tunning parameters are inspired by the discussion under the Kaggle challenge: https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment/\n",
    "\n",
    "\n",
    "I selected about 5000 features, and used softmax function for activation function instead of the traditional reLU on text for we are using the embeddings of categorical variables. \n",
    "\n",
    "I trained the model on 12 epochs, and we can see from the training progress output there, the 2nd epoch's accuracy already above 80%. While the 12th epoch has the accuracy of 94.5%. \n",
    "\n",
    "In the end we evaluated the trained model against the testing dataset, and got 93.6% accuracy - not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qinxi/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/preprocessing/text.py:157: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=5000, split=' ')\n",
    "tokenizer.fit_on_texts(tweet.text.values)\n",
    "X = tokenizer.texts_to_sequences(tweet.text.values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64943, 241)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qinxi/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(200, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5000, 241 ,input_length = X.shape[1]))\n",
    "model.add(LSTM(200, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(tweet.emo).values\n",
    "X_lstm_train, X_lstm_test, Y_lstm_train, Y_lstm_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n",
    "# print(X_train.shape,Y_train.shape)\n",
    "# print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      " - 2915s - loss: 0.4233 - acc: 0.7892\n",
      "Epoch 2/12\n",
      " - 2913s - loss: 0.3553 - acc: 0.8279\n",
      "Epoch 3/12\n",
      " - 2910s - loss: 0.3147 - acc: 0.8505\n",
      "Epoch 4/12\n",
      " - 2910s - loss: 0.2784 - acc: 0.8672\n",
      "Epoch 5/12\n",
      " - 2922s - loss: 0.2428 - acc: 0.8840\n",
      "Epoch 6/12\n",
      " - 2933s - loss: 0.2133 - acc: 0.8990\n",
      "Epoch 7/12\n",
      " - 2916s - loss: 0.1869 - acc: 0.9117\n",
      "Epoch 8/12\n",
      " - 2917s - loss: 0.1666 - acc: 0.9218\n",
      "Epoch 9/12\n",
      " - 2913s - loss: 0.1511 - acc: 0.9283\n",
      "Epoch 10/12\n",
      " - 2921s - loss: 0.1384 - acc: 0.9354\n",
      "Epoch 11/12\n",
      " - 2914s - loss: 0.1262 - acc: 0.9408\n",
      "Epoch 12/12\n",
      " - 2923s - loss: 0.1199 - acc: 0.9447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3c08a438>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_lstm_train, Y_lstm_train, epochs = 12, batch_size=32, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc, score = model.evaluate(X_lstm_test, Y_lstm_test, batch_size = 32, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.936\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy is %.3f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendex\n",
    "\n",
    "Here are the queries I ran to get the tweets. For security reason, I have removed the API token and keys.\n",
    "\n",
    "Credit to many stackoverflow posts over setting the histrical time limit and Tweep overtime error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def load_api():\n",
    "  \n",
    "    consumer_key = ''\n",
    "    consumer_secret = ''\n",
    "    access_token = ''\n",
    "    access_secret = ''\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    return tweepy.API(auth)\n",
    "\n",
    "def tweet_search():\n",
    "    search_phrases = ['#MeToo']\n",
    "    time_limit = 1                           \n",
    "    max_tweets = 2000                          \n",
    "    min_days_old, max_days_old = 1, 7         \n",
    "    USA = '39.8,-95.583068847656,2500km'\n",
    "    api = load_api()\n",
    "    for search_phrase in search_phrases:\n",
    "        print('Search phrase =', search_phrase)\n",
    "        name = search_phrase.split()[0]\n",
    "        json_file_root = name + '/'  + name\n",
    "        os.makedirs(os.path.dirname(json_file_root), exist_ok=True)\n",
    "\n",
    "        if max_days_old - min_days_old == 1:\n",
    "            d = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "            day = '{0}-{1:0>2}-{2:0>2}'.format(d.year, d.month, d.day)\n",
    "        else:\n",
    "            d1 = dt.datetime.now() - dt.timedelta(days=max_days_old-1)\n",
    "            d2 = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "            day = '{0}-{1:0>2}-{2:0>2}_to_{3}-{4:0>2}-{5:0>2}'.format(\n",
    "                  d1.year, d1.month, d1.day, d2.year, d2.month, d2.day)\n",
    "        json_file = json_file_root + '_' + day + '.json'\n",
    "\n",
    "\n",
    "        start = dt.datetime.now()\n",
    "        end = start + dt.timedelta(hours=3)\n",
    "        while dt.datetime.now() < end:\n",
    "            tweets, max_id = tweet_search(api, search_phrase, max_tweets,\n",
    "                                          max_id=max_id, since_id=since_id,geocode=USA)\n",
    "                searched_tweets = []\n",
    "            while len(searched_tweets) < max_tweets:\n",
    "                remaining_tweets = max_tweets - len(searched_tweets)\n",
    "                    try:\n",
    "                        new_tweets = api.search(q=query, count=remaining_tweets,\n",
    "                                    since_id=str(since_id), max_id=str(max_id-1))\n",
    "\n",
    "                        print('found',len(new_tweets),'tweets')\n",
    "                        if not new_tweets:\n",
    "                            break\n",
    "                    searched_tweets.extend(new_tweets)\n",
    "                    max_id = new_tweets[-1].id\n",
    "                    except tweepy.TweepError:\n",
    "                        print('exception raised, waiting 15 minutes')\n",
    "                        print('(until:', dt.datetime.now()+dt.timedelta(minutes=15), ')')\n",
    "                        time.sleep(15*60)\n",
    "                        break # stop the loop\n",
    "                    if tweets:\n",
    "                        with open(json_file, 'a') as f:\n",
    "                            for tweet in tweets:\n",
    "                                json.dump(tweet.text, f)\n",
    "                                    f.write('\\n')\n",
    "        print(\"done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
